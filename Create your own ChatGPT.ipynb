{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirmotefaker/create-your-own-chatgpt?scriptVersionId=120118437\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"ef2c6341","metadata":{"_execution_state":"idle","_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","papermill":{"duration":0.002794,"end_time":"2023-02-23T21:23:16.012821","exception":false,"start_time":"2023-02-23T21:23:16.010027","status":"completed"},"tags":[]},"source":["# Introduction\n","- ChatGPT (Chat Generative Pre-trained Transformer) is an AI-powered chatbot created by [OpenAI](https://openai.com/) that enables users to have highly sophisticated, human-like conversations. The language model is capable of answering questions and assist in various tasks, including writing emails, essays, and code. Due to its dialogue design, ChatGPT is capable of answering follow-up questions, acknowledging errors, questioning incorrect assumptions, and declining inappropriate requests.\n","\n","- The ChatGPT model was fine-tuned from a model in the GPT-3.5 series, which completed its training in early 2022. The ChatGPT as well as the related GPT-3.5 models were trained on a high-performance Azure AI supercomputing infrastructure.\n","\n","- While ChatGPT possesses many strengths, being a generalized model, it may not always be the most effective solution for narrower, more specialized topics with limited training data available. Moreover, the dialog interface has not yet been made available by OpenAI for businesses to integrate.\n","\n","[OpenAI Python Library Repo](https://github.com/openai/openai-python)\n","\n","[OpenAI Python Library Website](https://platform.openai.com/docs/libraries)"]},{"cell_type":"markdown","id":"f764eaa1","metadata":{"papermill":{"duration":0.001704,"end_time":"2023-02-23T21:23:16.016632","exception":false,"start_time":"2023-02-23T21:23:16.014928","status":"completed"},"tags":[]},"source":["# GPT-3\n","- In 2020, the Generative Pre-trained Transformer 3 (GPT-3) was introduced as an autoregressive language model capable to generate high-quality text that resembles human writing. The GPT-3 is the third generation of the GPT language models made available by OpenAI.\n","\n","- By providing an initial prompt as input, GPT-3 has the ability to produce a continuation of the text that follows the style and structure of the input prompt. The model is capable of performing a range of tasks, including but not limited to, text classification, question answering, text generation, text summarization, named-entity recognition, and language translation."]},{"cell_type":"markdown","id":"46d49462","metadata":{"papermill":{"duration":0.001639,"end_time":"2023-02-23T21:23:16.020236","exception":false,"start_time":"2023-02-23T21:23:16.018597","status":"completed"},"tags":[]},"source":["# ChatGPT Methods\n","\n","- We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as [InstructGPT](https://openai.com/blog/instruction-following/), but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format.\n","\n","- Reinforcement learning from human feedback enhances the RL agent's training by including humans in the training process. This helps account for the elements that can't be measured in the reward system.\n","\n","- To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/). We performed several iterations of this process.\n","\n","- ChatGPT is fine-tuned from a model in the GPT-3.5 series, which finished training in early 2022. You can learn more about the 3.5 series [here](https://beta.openai.com/docs/model-index-for-researchers). ChatGPT and GPT 3.5 were trained on an Azure AI supercomputing infrastructure."]},{"cell_type":"markdown","id":"fa67b11d","metadata":{"papermill":{"duration":0.001693,"end_time":"2023-02-23T21:23:16.023975","exception":false,"start_time":"2023-02-23T21:23:16.022282","status":"completed"},"tags":[]},"source":["# ChatGPT Limitations\n","- ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers. Fixing this issue is challenging, as: during RL training, there’s currently no source of truth; training the model to be more cautious causes it to decline questions that it can answer correctly; and supervised training misleads the model because the ideal answer [depends on what the model knows](https://www.alignmentforum.org/posts/BgoKdAzogxmgkuuAt/behavior-cloning-is-miscalibrated), rather than what the human demonstrator knows.\n","\n","- ChatGPT is sensitive to tweaks to the input phrasing or attempting the same prompt multiple times. For example, given one phrasing of a question, the model can claim to not know the answer, but given a slight rephrase, can answer correctly.\n","\n","- The model is often excessively verbose and overuses certain phrases, such as restating that it’s a language model trained by OpenAI. These issues arise from biases in the training data (trainers prefer longer answers that look more comprehensive) and well-known over-optimization issues.\n","\n","- Ideally, the model would ask clarifying questions when the user provided an ambiguous query. Instead, our current models usually guess what the user intended.\n","\n","- While we’ve made efforts to make the model refuse inappropriate requests, it will sometimes respond to harmful instructions or exhibit biased behavior. We’re using the [Moderation API](https://openai.com/blog/new-and-improved-content-moderation-tooling/) to warn or block certain types of unsafe content, but we expect it to have some false negatives and positives for now. We’re eager to collect user feedback to aid our ongoing work to improve this system."]},{"cell_type":"markdown","id":"cdb059a8","metadata":{"papermill":{"duration":0.001659,"end_time":"2023-02-23T21:23:16.027525","exception":false,"start_time":"2023-02-23T21:23:16.025866","status":"completed"},"tags":[]},"source":["# Model index\n","- Our models are used for both research purposes and developer use cases in production. Researchers often learn about our models from papers that we have published, but there is often not a perfect match between what is available in the OpenAI API and what is published in a paper.\n","\n","- The purpose of this page is to help clarify:\n","\n","  - Some of the differences in the ways that our models are trained, which impacts the comparisons that can be made between models, and various evaluation results.\n","  - The differences between various model series, such as GPT 3.5 and InstructGPT.\n","  - Which if any of the models available in the API today match with a model in a paper. In some cases, there might not be a match."]},{"cell_type":"markdown","id":"84e561db","metadata":{"papermill":{"duration":0.001645,"end_time":"2023-02-23T21:23:16.030981","exception":false,"start_time":"2023-02-23T21:23:16.029336","status":"completed"},"tags":[]},"source":["# Models referred to as \"GPT 3.5\"\n","- GPT-3.5 series is a series of models that was trained on a blend of text and code from before Q4 2021. The following models are in the GPT-3.5 series:\n","\n","  - code-davinci-002 is a base model, so good for pure code-completion tasks\n","  - text-davinci-002 is an InstructGPT model based on code-davinci-002\n","  - text-davinci-003 is an improvement on text-davinci-002"]},{"cell_type":"markdown","id":"014adec6","metadata":{"papermill":{"duration":0.003634,"end_time":"2023-02-23T21:23:16.036483","exception":false,"start_time":"2023-02-23T21:23:16.032849","status":"completed"},"tags":[]},"source":["# TRAINING METHOD\tMODELS\n","  - SFT\n","    - Supervised fine-tuning on human demonstrations\tdavinci-instruct-beta1\n","\n","  - FeedME\n","    - Supervised fine-tuning on human-written demonstrations and on model samples rated 7/7 by human labelers on an overall quality score\ttext-davinci-001, text-davinci-002, text-curie-001, text-babbage-001\n","\n","  - PPO\n","    - Reinforcement learning with reward models trained from comparisons by humans\ttext-davinci-003\n","\n","#### The SFT and PPO models are trained similarly to the ones from the [InstructGPT paper](https://arxiv.org/abs/2203.02155). FeedME (short for \"feedback made easy\") models are trained by distilling the best completions from all of our models. Our models generally used the best available datasets at the time of training, and so different engines using the same training methodology might be trained on different data."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":8.60074,"end_time":"2023-02-23T21:23:16.759577","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-02-23T21:23:08.158837","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}